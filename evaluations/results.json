[
    {
        "accuracy": 32.587859424920126,
        "number_of_samples": 100,
        "model_name": "llama3.2:3B"
    },
    {
        "accuracy": 32.86219081272085,
        "number_of_samples": 100,
        "model_name": "llama3.2:3B"
    },
    {
        "accuracy": 54.632587859424916,
        "number_of_samples": 100,
        "model_name": "Isotonic/deberta-v3-base_finetuned_ai4privacy_v2"
    },
    {
        "accuracy": 49.774173074936165,
        "number_of_samples": 43501,
        "model_name": "Isotonic/deberta-v3-base_finetuned_ai4privacy_v2"
    },
    {
        "accuracy": 4.15335463258786,
        "number_of_samples": 100,
        "model_name": "gemma:2b"
    },
    {
        "accuracy": 38.33865814696485,
        "number_of_samples": 100,
        "model_name": "qwen2.5:3b"
    },
    {
        "accuracy": 31.629392971246006,
        "number_of_samples": 100,
        "model_name": "llama3.2:latest"
    },
    {
        "accuracy": 38.019169329073485,
        "number_of_samples": 100,
        "model_name": "updated-llama3.2:latest"
    },
    {
        "accuracy": 22.683706070287542,
        "number_of_samples": 100,
        "model_name": "deepseek-r1:1.5b"
    },
    {
        "accuracy": 84.05985686402082,
        "number_of_samples": 500,
        "model_name": "updated-llama3.2:latest"
    },
    {
        "accuracy": 71.43786597267405,
        "number_of_samples": 500,
        "model_name": "qwen2.5:3b"
    },
    {
        "accuracy": 71.43786597267405,
        "number_of_samples": 100,
        "model_name": "updated-llama3.2:latest"
    },
    {
        "accuracy": 72.20447284345049,
        "number_of_samples": 100,
        "model_name": "qwen2.5:3b"
    },
    {
        "accuracy": 20.766773162939298,
        "number_of_samples": 100,
        "model_name": "gemma:2b"
    },
    {
        "accuracy": 40.2555910543131,
        "number_of_samples": 100,
        "model_name": "deepseek-r1:1.5b"
    },
    {
        "accuracy": 33.37670787247885,
        "number_of_samples": 500,
        "model_name": "deepseek-r1:1.5b"
    },
    {
        "accuracy": 93.10344827586206,
        "number_of_samples": 10,
        "model_name": "llama_model_new:latest",
        "average_time_per_prompt": 24.06,
        "total_time": 240.63
    },
    {
        "accuracy": 93.10344827586206,
        "number_of_samples": 10,
        "model_name": "llama_model_new:latest",
        "average_time_per_prompt": 24.06,
        "total_time": 240.63
    },
    {
        "accuracy": 82.10862619808307,
        "number_of_samples": 100,
        "model_name": "llama_model_new:latest",
        "average_time_per_prompt": 16.22,
        "total_time": 1922.28
    },
    {
        "accuracy": 84.05985686402082,
        "number_of_samples": 500,
        "model_name": "llama_model_new:latest",
        "average_time_per_prompt": 13.59,
        "total_time": 7995.29
    },
    {
        "accuracy": 73.78009108653221,
        "number_of_samples": 500,
        "model_name": "qwen2.5:3b",
        "average_time_per_prompt": 8.81,
        "total_time": 4407.2
    },
    {
        "accuracy": 72.52396166134186,
        "number_of_samples": 100,
        "model_name": "qwen2.5:3b",
        "average_time_per_prompt": 9.28,
        "total_time": 927.75
    },
    {
        "accuracy": 86.20689655172413,
        "number_of_samples": 10,
        "model_name": "qwen2.5:3b",
        "average_time_per_prompt": 9.63,
        "total_time": 96.36
    }
]